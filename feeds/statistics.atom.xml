<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Notebooks</title><link href="http://nb.asorge.de/" rel="alternate"></link><link href="http://nb.asorge.de/feeds/statistics.atom.xml" rel="self"></link><id>http://nb.asorge.de/</id><updated>2014-09-02T00:00:00+02:00</updated><entry><title>Confidence Intervals for Binomial Proportions</title><link href="http://nb.asorge.de/confidence-intervals-for-binomial-proportions.html" rel="alternate"></link><updated>2014-09-02T00:00:00+02:00</updated><author><name></name></author><id>tag:nb.asorge.de,2014-09-02:confidence-intervals-for-binomial-proportions.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.ipython.org/github/andsor/notebooks/blob/master/notebooks/confidence-intervals-for-binomial-proportions.ipynb"&gt;View this notebook in
nbviewer&lt;/a&gt; | &lt;a href="https://github.com/andsor/notebooks/raw/master/notebooks/confidence-intervals-for-binomial-proportions.pdf"&gt;Get
PDF&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="confidence-intervals-for-binomial-proportions"&gt;Confidence Intervals for Binomial Proportions&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
InÂ []:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats.distributions&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dist&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Consider a discrete random variable $X$ which indicates either "success"
($X=1$) or "failure" ($X=0$) as the outcome of a random experiment.
Such an experiment is called a &lt;em&gt;Bernoulli trial&lt;/em&gt;.
The probability of success is $p=P\{X=1\}$, and the probability of failure is
$P\{X=0\}=1-p$.
Repeating a Bernoulli trial $n$ times means drawing a sample of $n$ independent
and identically distributed random variables $X_i$.
The probability mass function of observing $k$ success is the binomial
distribution&lt;/p&gt;
&lt;p&gt;$$
\binom{n}{k} p^k (1-p)^{n-k}.
$$&lt;/p&gt;
&lt;p&gt;The expected number of successes is $np$ with variance $np(1-p)$.
Hence, the success probability $p$ is the expected &lt;em&gt;proportion&lt;/em&gt; of successes
$\frac{k}{n}$, with variance $p(1-p)/n$.&lt;/p&gt;
&lt;p&gt;Let $\hat{p}:=\frac{k}{n}$ denote the sample proportion which is the unbiased
(and maximum likelihood) estimator for the success probability $p$, and let
$\hat{\sigma} := \hat{p}(1-\hat{p})/n$ denote the sample variance.
Then the normal $1-\alpha$ confidence interval for the binomial proportion
$\hat{p}$ is&lt;/p&gt;
&lt;p&gt;$$
\hat{p} \pm z_{\alpha/2} \hat{\sigma},
$$&lt;/p&gt;
&lt;p&gt;where $z_{\alpha/2}$ is the $1 - \frac{\alpha}{2}$ quantile of the standard
normal distribution.
This normal confidence interval is also called the &lt;em&gt;Wald confidence interval&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As Cameron &lt;cite data-cite="Cameron2011Estimation"&gt;(&lt;a href="http://dx.doi.org/10.1071/as10046"&gt;Cameron, 2011&lt;/a&gt;)&lt;/cite&gt;
puts it, this normal approximation "suffers a &lt;em&gt;systematic&lt;/em&gt; decline in
performance both for small $n$ and towards extreme values of $p$ near $0$ and
$1$, generating binomial [confidence intervals] with effective coverage far
below the desired level." (see also &lt;cite data-cite="Agresti1998Approximate"&gt;(&lt;a href="http://dx.doi.org/10.2307/2685469"&gt;Agresti &amp;amp; Coull, 1998&lt;/a&gt;)&lt;/cite&gt; and 
&lt;cite data-cite="DasGupta2001Interval"&gt;(&lt;a href="http://dx.doi.org/10.1214/ss/1009213286"&gt;DasGupta et al., 2001&lt;/a&gt;)&lt;/cite&gt;)&lt;/p&gt;
&lt;p&gt;A different approach to quantifying uncertainty is Bayesian inference.
The normal (frequentist) $1 - \alpha$ confidence interval derives from a
procedure that produces $1 - \alpha$ confidence intervals that contain the true
parameter value $100(1-\alpha)\%$ of the times.
The $1-\alpha$ credible interval of Bayesian inference is the interval in which
the parameter lies with probability $1-\alpha$. 
&lt;cite data-cite="Wasserman2004All"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-21736-9"&gt;Wasserman, 2004&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;Specifically, Bayesian inference employes Bayes' theorem
$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}.
$$
Associate $A$ with the parameter and $B$ with the outcome from an experiment
(the data).
Then $P(A)$ is the &lt;em&gt;prior&lt;/em&gt; probability of the parameter event $A$, with the
&lt;em&gt;likelihood&lt;/em&gt; $P(B|A)$ of the outcome event $B$ given the parameter event $A$.
The &lt;em&gt;posterior&lt;/em&gt; $P(A|B)$ is the probability of the parameter event $A$ given
the outcome event $B$.&lt;/p&gt;
&lt;p&gt;For probability density functions, this reads
$$
f(\theta|x) = \frac{f(x|\theta) f(\theta)}{\int d\theta f(\theta|x)f(\theta)}
$$
with parameter $\theta$ and data $x$.
A Bayesian interval estimate is the $1 - \alpha$ &lt;em&gt;posterior interval&lt;/em&gt; or
&lt;em&gt;credible interval&lt;/em&gt; $(l,u)$ with $\int_{-\infty}^l d\theta f(\theta|x) =
\int_{u}^{+\infty} d\theta f(\theta | x) = \alpha/2$ such that $P(\theta \in
(l,u)|x) = 1 - \alpha$.&lt;/p&gt;
&lt;p&gt;For $n$ independent Bernoulli trials with common success probability $p$, the
&lt;em&gt;likelihood&lt;/em&gt; to have $k$ successes given $p$ is the binomial distribution
$$
P(k|p) = \binom{n}{k} p^k (1-p)^{n-k} \equiv B(a,b),
$$
where $B(a,b)$ is the &lt;em&gt;Beta distribution&lt;/em&gt; with parameters $a = k+1$ and $b = n
- k + 1$.
Assuming a uniform prior $P(p) = 1$, the &lt;em&gt;posterior&lt;/em&gt; is &lt;cite data-cite="Wasserman2004All"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-21736-9"&gt;Wasserman, 2004&lt;/a&gt;)&lt;/cite&gt;
$$
P(p|k) = P(k|p)=B(a,b).
$$
A point estimate is the posterior mean
$$
\bar{p} = \frac{k+1}{n+2}
$$
with $1 - \alpha$ credible interval $(p_l, p_u)$ given by
$$
\int_0^{p_l} dp B(a,b) = \int_{p_u}^1 dp B(a,b) = \frac{\alpha}{2}.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="http://notebooks.asorge.de"&gt;This work&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative
Commons Attribution 4.0 International
License&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</summary><category term="statistics"></category><category term="diss"></category></entry><entry><title>The Quality of Finite-Size Data Collapse</title><link href="http://nb.asorge.de/quality-of-finite-size-data-collapse.html" rel="alternate"></link><updated>2014-09-02T00:00:00+02:00</updated><author><name></name></author><id>tag:nb.asorge.de,2014-09-02:quality-of-finite-size-data-collapse.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.ipython.org/github/andsor/notebooks/blob/master/notebooks/quality-of-data-collapse.ipynb"&gt;View this notebook in nbviewer&lt;/a&gt; | &lt;a href="https://github.com/andsor/notebooks/raw/master/notebooks/quality-of-data-collapse.pdf"&gt;Get
PDF&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="the-quality-of-finite-size-data-collapse"&gt;The Quality of Finite-Size Data Collapse&lt;/h1&gt;
&lt;h1 id="finite-size-scaling"&gt;Finite-size scaling&lt;/h1&gt;
&lt;p&gt;The finite-size scaling ansatz &lt;/p&gt;
&lt;p&gt;$$
A_L(\varrho) = L^{\zeta/\nu} \tilde{f}\left(L^{1/\nu} (\varrho - \varrho_c)
\right)
$$&lt;/p&gt;
&lt;p&gt;postulates how a physical quantity $A_L(\varrho)$ observed in a system of
finite size scales with system size $L$ and paramater $\varrho$ according to a
scaling function $\tilde{f}$, the critical parameter $\varrho_c$, the
critical exponent $\zeta$ of the quantity itself, and the critical exponent
$\nu$ of the correlation length $\xi$ 
&lt;cite data-cite="Newman1999Monte"&gt;([Newman &amp;amp; Barkema, 1999])&lt;/cite&gt;, 
&lt;cite data-cite="Binder2010Monte"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-3-642-03163-2"&gt;Binder &amp;amp; Heermann, 2010&lt;/a&gt;)&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Finite-size scaling analysis concerns experimental data $a_{L_i, \varrho_j}$ at
system sizes $L_i$ and parameter values $\varrho_j$.
Plotting $L_i^{-\zeta/\nu} a_{L_i, \varrho_j}$ against $L_i^{1/nu} (\varrho -
\varrho_c)$ with the right choice of $\varrho_c, \nu, \zeta$ should let the
data collapse onto a single curve.
The single curve of course is the scaling function $\tilde{f}$ from the
finite-size scaling ansatz.
In the following, we present a measure by Houdayer &amp;amp; Hartmann 
&lt;cite data-cite="Houdayer2004Lowtemperature"&gt;(&lt;a href="http://dx.doi.org/10.1103/physrevb.70.014418"&gt;Houdayer &amp;amp; Hartmann, 2004&lt;/a&gt;)&lt;/cite&gt;
for the quality of the data collapse.
Melchert &lt;cite data-cite="Melchert2009AutoScalepy"&gt;(&lt;a href="http://arxiv.org/abs/0910.5403"&gt;Melchert, 2009&lt;/a&gt;)&lt;/cite&gt;
refers to some alternative measures, for example &lt;cite data-cite="Bhattacharjee2001Measure"&gt;(&lt;a href="http://dx.doi.org/10.1088/0305-4470/34/33/302"&gt;Bhattacharjee &amp;amp; Seno, 2001&lt;/a&gt;)&lt;/cite&gt;,
&lt;cite data-cite="Wenzel2008Percolation"&gt;(&lt;a href="http://dx.doi.org/10.1016/j.nuclphysb.2007.10.024"&gt;Wenzel et al., 2008&lt;/a&gt;)&lt;/cite&gt;, and
to some applications of these measures in the literature.&lt;/p&gt;
&lt;h1 id="the-quality-function"&gt;The quality function&lt;/h1&gt;
&lt;p&gt;Houdayer &amp;amp; Hartmann &lt;cite data-cite="Houdayer2004Lowtemperature"&gt;(&lt;a href="http://dx.doi.org/10.1103/physrevb.70.014418"&gt;Houdayer &amp;amp;
Hartmann, 2004&lt;/a&gt;)&lt;/cite&gt; refine a method proposed by Kawashima &amp;amp; Ito 
&lt;cite data-cite="Kawashima1993Critical"&gt;(&lt;a href="http://dx.doi.org/10.1143/jpsj.62.435"&gt;Kawashima &amp;amp; Ito, 1993&lt;/a&gt;)&lt;/cite&gt;.
They define the quality as the reduced $\chi^2$ statistic&lt;/p&gt;
&lt;p&gt;\begin{equation}
S = \frac{1}{\mathcal{N}} \sum_{i,j} \frac{(y_{ij} -
Y_{ij})^2}{dy_{ij}^2+dY_{ij}^2},
\end{equation}&lt;/p&gt;
&lt;p&gt;where the values $y_{ij}, dy_{ij}$ are the scaled observations and its standard
errors at $x_{ij}$, and the values $Y_{ij}, dY_{ij}$ are the estimated value of
the master curve and its standard error at $x_{ij}$.&lt;/p&gt;
&lt;p&gt;The quality $S$ is the mean square of the weighted deviations from the master
curve.
As we expect the individual deviations $y_{ij} - Y_{ij}$ to be of the order of
the individual error $\sqrt{dy_{ij}^2 + dY_{ij}^2}$ for an optimal fit, the
quality $S$ should attain its minimum $S_{\min}$ at around $1$ and be much
larger otherwise &lt;cite data-cite="Bevington2003Data"&gt;([Bevington &amp;amp; Robinson,
2003])&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Let $i$ enumerate the system sizes $L_i$, $i = 1, \ldots, k$ and let $j$
enumerate the parameters $\varrho_j$, $j = 1, \ldots, n$ with $\varrho_1 &amp;lt;
\varrho_2 &amp;lt; \ldots &amp;lt; \varrho_n$.
The scaled data are&lt;/p&gt;
&lt;p&gt;\begin{align}
y_{ij} &amp;amp; := L_i^{-\zeta/\nu} a_{L_i, \varrho_j} \\
dy_{ij} &amp;amp; := L_i^{-\zeta/\nu} da_{L_i, \varrho_j} \\
x_{ij}  &amp;amp; := L_i^{1/\nu}(\varrho_j - \varrho_c).
\end{align}&lt;/p&gt;
&lt;p&gt;The sum in the quality function $S$ only involves terms for which the estimated
value $Y_{ij}$ of the master curve at $x_{ij}$ is defined. The number of such
terms is $\mathcal{N}$.&lt;/p&gt;
&lt;p&gt;The master curve itself depends on the scaled data. For a given $i$, $L_i$, we
estimate the master curve at $x_{ij}$ by the two respective data from all the
other system sizes which respectively enclose $x_{ij}$:
for each $i \neq i$, let $j'$ be such that $x_{i'j'} \leq x_{ij} \leq
x_{i'(j'+1)}$, and select the points $(x_{i'j'}, y_{i'j'}, dy_{i'j'}),
(x_{i'(j'+1)}, y_{i'(j'+1)}, dy_{i'(j'+1)})$.
Do not select points for some $i'$, if there is no such $j'$. If there is no
such $j'$ for all $i'$, the master curve remains undefined at $x_{ij}$.&lt;/p&gt;
&lt;p&gt;Given the selected points $(x_l, y_l, dy_l)$, the local approximation of the
master curve is the linear fit&lt;/p&gt;
&lt;p&gt;$$
y = mx + b
$$&lt;/p&gt;
&lt;p&gt;with weighted least squares &lt;cite data-cite="Strutz2011Data"&gt;(Strutz,
2011)&lt;/cite&gt;.
The weights $w_l$ are the reciprocal variances, $w_l := 1/dy_{ij}^2$.
The estimates and (co)variances of the slope $m$ and intercept $b$ are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{b} &amp;amp;= \frac{1}{\Delta} (K_{xx}K_y - K_xK_{xy}) \\
\hat{m} &amp;amp;= \frac{1}{\Delta} (K K_{xy} - K_x K_y)
\end{align*}&lt;/p&gt;
&lt;p&gt;$$
\hat{\sigma}_b^2 = \frac{K_{xx}}{\Delta} , \hat{\sigma}_m^2 = \frac{K}{\Delta},
\hat{\sigma}_{bm} = - \frac{K_x}{\Delta}
$$&lt;/p&gt;
&lt;p&gt;with $K_{nm} := \sum w_l x_l^n y_l^m$, $K := K_{00}$, $K_x := K_{10}$, $K_y :=
K_{01}$, $K_{xx} := K_{20}$, $K_{xy} := K_{11}$, $\Delta := KK_{xx} - K_x^2$.&lt;/p&gt;
&lt;p&gt;Hence, the estimated value of the master curve at $x_{ij}$ is&lt;/p&gt;
&lt;p&gt;$$
Y_{ij} = \hat{m} x_{ij} + \hat{b}
$$&lt;/p&gt;
&lt;p&gt;with error propagation&lt;/p&gt;
&lt;p&gt;$$
dY_{ij}^2 = \hat{\sigma}^2 x_{ij}^2 + 2 \hat{\sigma}_{bm} x_{ij} +
\hat{\sigma}_b^2.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="http://notebooks.asorge.de"&gt;This work&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative
Commons Attribution 4.0 International
License&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</summary><category term="diss"></category><category term="data_analysis"></category><category term="statistics"></category></entry><entry><title>Sectioning &amp; Bootstrapping</title><link href="http://nb.asorge.de/sectioning-and-bootstrapping.html" rel="alternate"></link><updated>2014-09-02T00:00:00+02:00</updated><author><name></name></author><id>tag:nb.asorge.de,2014-09-02:sectioning-and-bootstrapping.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.ipython.org/github/andsor/notebooks/blob/master/notebooks/sectioning-and-bootstrapping.ipynb"&gt;View this notebook in
nbviewer&lt;/a&gt; | &lt;a href="https://github.com/andsor/notebooks/raw/master/notebooks/sectioning-and-bootstrapping.pdf"&gt;Get
PDF&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="sectioning-and-bootstrapping"&gt;Sectioning and bootstrapping&lt;/h1&gt;
&lt;p&gt;The following presentation is based on the excellent textbook by Asmussen and
Glynn &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Given a random element $X$, its distribution $F$, and some real-valued
functional $\psi$, we would like to estimate $\psi(F)$ and its $1-\alpha$
confidence interval without further assumptions.
For example, the mean is the
functional $\psi(F) = \int x F(dx)$, where $dx$ is the probability to find $X$
in $dx$.&lt;/p&gt;
&lt;p&gt;Given $R$ independent samples $X_1, \ldots, X_R$ from $F$, an estimate for
$\psi(F)$ is $\psi(\hat{F}_R)$, where&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_R(dx) := \frac{1}{R} \sum_{r=1}^R \delta_{X_r}(dx)
$$&lt;/p&gt;
&lt;p&gt;is the &lt;em&gt;empirical distribution&lt;/em&gt; and $\delta_{X_r}(A) = 1 \Leftrightarrow X_r
\in A$.&lt;/p&gt;
&lt;p&gt;For real-valued random variables, the empirical cumulative distribution
function is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_R(x) := \frac{1}{R} \sum_{r=1}^R \mathbb{1}_{\{X_r \leq x \}}.
$$&lt;/p&gt;
&lt;p&gt;As $R \to \infty$, we have $\psi(\hat{F}_R) \to \psi(F)$ almost surely &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn, 2007&lt;/a&gt;)&lt;/cite&gt;.
Furthermore, we have a central limit theorem such that $\psi(\hat{F}_R)$ is
distributed as $\psi(F) + Y$, where $Y \sim \mathcal{N}(0, \sigma/\sqrt{R})$.&lt;/p&gt;
&lt;h2 id="sectioning"&gt;Sectioning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Sectioning&lt;/em&gt; means splitting the sample into $N$ subsamples (sections) of size
$K$. The empirical distribution of the $n$-th section is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_{n,K}(dx) := \frac{1}{K} \sum_{r=(n-1)K + 1}^{nK} \delta_{X_r}(dx).
$$&lt;/p&gt;
&lt;p&gt;The $1 - \alpha$ confidence interval for the estimator $\psi(\hat{F})$ is&lt;/p&gt;
&lt;p&gt;$$
\psi(\hat{F}) \pm t_{1-\alpha/2} \frac{\hat{\sigma}}{\sqrt{N}},
$$&lt;/p&gt;
&lt;p&gt;where $t_{1-\alpha/2}$ is the critical value of the Student t distribution with
$N-1$ degrees of freedom and the estimator for the variance&lt;/p&gt;
&lt;p&gt;$$
\hat{\sigma}^2 := \frac{1}{N-1} \sum_{n=1}^N \left(\psi(\hat{F}_{n,K}) -
\psi(\hat{F}_R) \right)^2.
$$&lt;/p&gt;
&lt;p&gt;The number of sections needs to be sufficiently large in order for the central
limit theorem to approximately hold.&lt;/p&gt;
&lt;h2 id="bootstrapping"&gt;Bootstrapping&lt;/h2&gt;
&lt;p&gt;When a model for the distribution $F$ is lacking, or too complicated for
statistical inference, bootstrapping methods provide alternatives.
Bootstrapping takes the empirical distribution $\hat{F}_R$ as a surrogate for
the true distribution $F$.
Instead of drawing more samples from $F$, bootstrapping involves resampling
from $\hat{F}_R$.&lt;/p&gt;
&lt;p&gt;The true $1 - \alpha$ confidence interval of the estimator $\psi(\hat{F}_R)$ is&lt;/p&gt;
&lt;p&gt;$$
(\psi(\hat{F}_R) - z_2, \psi(\hat{F}_R) - z_1)
$$&lt;/p&gt;
&lt;p&gt;with the $\alpha/2$ and $1 - \alpha/2$ quantiles $z_1, z_2$&lt;/p&gt;
&lt;p&gt;$$
P(\psi(\hat{F}_R) - \psi(F) &amp;lt; z_1) = P(\psi(\hat{F}_R) - \psi(F) &amp;gt; z_2) =
\frac{\alpha}{2}
$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$
P(\psi(F) \in (\psi(\hat{F}_R) - z_2, \psi(\hat{F}_R) - z_1)) = 1 - \alpha.
$$&lt;/p&gt;
&lt;p&gt;Assuming that $\hat{F}_R \approx F$, the empirical quantiles $z_1^*, z_2^*$
satisfy &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;$$
P_{\hat{F}_R}(\psi(\hat{F}_R) - \psi(F) &amp;lt; z_1^*) =
P_{\hat{F}_R}(\psi(\hat{F}_R) - \psi(F) &amp;gt; z_2^*) =
\frac{\alpha}{2}
$$&lt;/p&gt;
&lt;p&gt;We draw $B$ bootstrap samples of size $R$ from $\hat{F}_R$.
The $b$-th bootstrap sample is $X_{1,b}^*, \ldots, X_{R,b}^*$ with each random
variable $X_{r,b}^*$ drawn independently from $X_1, \ldots, X_R$ with equal
probabilities $P(X_{r,b}^* = X_{r'}) = \frac{1}{R}$.
The empirical distribution of the $b$-th bootstrap sample is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_{R,b}^*(dx) := \frac{1}{R} \sum_{r=1}^R \delta_{X_{r,b}^*}(dx).
$$&lt;/p&gt;
&lt;p&gt;Then the empirical quantiles $z_1^*, z_2^*$ are the $\lfloor
\frac{\alpha}{2}(B+1) \rfloor$-th and $\lfloor (1 - \frac{\alpha}{2})(B+1)
\rfloor$-th order statistic, respectively, of the $B$ independent and
identically distributed random variables&lt;/p&gt;
&lt;p&gt;$$
\left( \psi(\hat{F}_{R,b}^*) - \psi(\hat{F}_R) \right)_{b=1}^B.
$$&lt;/p&gt;
&lt;p&gt;These quantiles $z_1^*, z_2^*$ approximate the quantiles $z_1, z_2$ of the true
distribution $F$, and hence, yield the approximate $1 - \alpha$ confidence
interval &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;$$
\left( \psi(\hat{F}_R) - z_2^*, \psi(\hat{F}_R) - z_1^* \right).
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="http://notebooks.asorge.de"&gt;This work&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative
Commons Attribution 4.0 International
License&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</summary><category term="diss"></category><category term="statistics"></category></entry></feed>