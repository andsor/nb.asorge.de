<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Notebooks</title><link href="http://nb.asorge.de/" rel="alternate"></link><link href="http://nb.asorge.de/feeds/statistics.atom.xml" rel="self"></link><id>http://nb.asorge.de/</id><updated>2014-08-25T00:00:00+02:00</updated><entry><title>Confidence Intervals for Binomial Proportions</title><link href="http://nb.asorge.de/confidence-intervals-for-binomial-proportions.html" rel="alternate"></link><updated>2014-08-25T00:00:00+02:00</updated><author><name></name></author><id>tag:nb.asorge.de,2014-08-25:confidence-intervals-for-binomial-proportions.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.ipython.org/github/andsor/notebooks/blob/master/notebooks/confidence-intervals-for-binomial-proportions.ipynb"&gt;View this notebook in
nbviewer&lt;/a&gt; | &lt;a href="https://github.com/andsor/notebooks/raw/master/notebooks/confidence-intervals-for-binomial-proportions.pdf"&gt;Get
PDF&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="confidence-intervals-for-binomial-proportions"&gt;Confidence Intervals for Binomial Proportions&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
InÂ []:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats.distributions&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dist&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Consider a discrete random variable $X$ which indicates either "success"
($X=1$) or "failure" ($X=0$) as the outcome of a random experiment.
Such an experiment is called a &lt;em&gt;Bernoulli trial&lt;/em&gt;.
The probability of success is $p=P\{X=1\}$, and the probability of failure is
$P\{X=0\}=1-p$.
Repeating a Bernoulli trial $n$ times means drawing a sample of $n$ independent
and identically distributed random variables $X_i$.
The probability mass function of observing $k$ success is the binomial
distribution&lt;/p&gt;
&lt;p&gt;$$
\binom{n}{k} p^k (1-p)^{n-k}.
$$&lt;/p&gt;
&lt;p&gt;The expected number of successes is $np$ with variance $np(1-p)$.
Hence, the success probability $p$ is the expected &lt;em&gt;proportion&lt;/em&gt; of successes
$\frac{k}{n}$, with variance $p(1-p)/n$.&lt;/p&gt;
&lt;p&gt;Let $\hat{p}:=\frac{k}{n}$ denote the sample proportion which is the unbiased
(and maximum likelihood) estimator for the success probability $p$, and let
$\hat{\sigma} := \hat{p}(1-\hat{p})/n$ denote the sample variance.
Then the normal $1-\alpha$ confidence interval for the binomial proportion
$\hat{p}$ is&lt;/p&gt;
&lt;p&gt;$$
\hat{p} \pm z_{\alpha/2} \hat{\sigma},
$$&lt;/p&gt;
&lt;p&gt;where $z_{\alpha/2}$ is the $1 - \frac{\alpha}{2}$ quantile of the standard
normal distribution.
This normal confidence interval is also called the &lt;em&gt;Wald confidence interval&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As Cameron &lt;cite data-cite="Cameron2011Estimation"&gt;(&lt;a href="http://dx.doi.org/10.1071/as10046"&gt;Cameron, 2011&lt;/a&gt;)&lt;/cite&gt;
puts it, this normal approximation "suffers a &lt;em&gt;systematic&lt;/em&gt; decline in
performance both for small $n$ and towards extreme values of $p$ near $0$ and
$1$, generating binomial [confidence intervals] with effective coverage far
below the desired level." (see also &lt;cite data-cite="Agresti1998Approximate"&gt;(&lt;a href="http://dx.doi.org/10.2307/2685469"&gt;Agresti &amp;amp; Coull, 1998&lt;/a&gt;)&lt;/cite&gt; and 
&lt;cite data-cite="DasGupta2001Interval"&gt;(&lt;a href="http://dx.doi.org/10.1214/ss/1009213286"&gt;DasGupta et al., 2001&lt;/a&gt;)&lt;/cite&gt;)&lt;/p&gt;
&lt;p&gt;A different approach to quantifying uncertainty is Bayesian inference.
The normal (frequentist) $1 - \alpha$ confidence interval derives from a
procedure that produces $1 - \alpha$ confidence intervals that contain the true
parameter value $100(1-\alpha)\%$ of the times.
The $1-\alpha$ credible interval of Bayesian inference is the interval in which
the parameter lies with probability $1-\alpha$. 
&lt;cite data-cite="Wasserman2004All"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-21736-9"&gt;Wasserman, 2004&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;Specifically, Bayesian inference employes Bayes' theorem
$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}.
$$
Associate $A$ with the parameter and $B$ with the outcome from an experiment
(the data).
Then $P(A)$ is the &lt;em&gt;prior&lt;/em&gt; probability of the parameter event $A$, with the
&lt;em&gt;likelihood&lt;/em&gt; $P(B|A)$ of the outcome event $B$ given the parameter event $A$.
The &lt;em&gt;posterior&lt;/em&gt; $P(A|B)$ is the probability of the parameter event $A$ given
the outcome event $B$.&lt;/p&gt;
&lt;p&gt;For probability density functions, this reads
$$
f(\theta|x) = \frac{f(x|\theta) f(\theta)}{\int d\theta f(\theta|x)f(\theta)}
$$
with parameter $\theta$ and data $x$.
A Bayesian interval estimate is the $1 - \alpha$ &lt;em&gt;posterior interval&lt;/em&gt; or
&lt;em&gt;credible interval&lt;/em&gt; $(l,u)$ with $\int_{-\infty}^l d\theta f(\theta|x) =
\int_{u}^{+\infty} d\theta f(\theta | x) = \alpha/2$ such that $P(\theta \in
(l,u)|x) = 1 - \alpha$.&lt;/p&gt;
&lt;p&gt;For $n$ independent Bernoulli trials with common success probability $p$, the
&lt;em&gt;likelihood&lt;/em&gt; to have $k$ successes given $p$ is the binomial distribution
$$
P(k|p) = \binom{n}{k} p^k (1-p)^{n-k} \equiv B(a,b),
$$
where $B(a,b)$ is the &lt;em&gt;Beta distribution&lt;/em&gt; with parameters $a = k+1$ and $b = n
- k + 1$.
Assuming a uniform prior $P(p) = 1$, the &lt;em&gt;posterior&lt;/em&gt; is &lt;cite data-cite="Wasserman2004All"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-21736-9"&gt;Wasserman, 2004&lt;/a&gt;)&lt;/cite&gt;
$$
P(p|k) = P(k|p)=B(a,b).
$$
A point estimate is the posterior mean
$$
\bar{p} = \frac{k+1}{n+2}
$$
with $1 - \alpha$ credible interval $(p_l, p_u)$ given by
$$
\int_0^{p_l} dp B(a,b) = \int_{p_u}^1 dp B(a,b) = \frac{\alpha}{2}.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="http://notebooks.asorge.de"&gt;This work&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative
Commons Attribution 4.0 International
License&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</summary><category term="statistics"></category><category term="diss"></category></entry><entry><title>Sectioning &amp; Bootstrapping</title><link href="http://nb.asorge.de/sectioning-and-bootstrapping.html" rel="alternate"></link><updated>2014-08-25T00:00:00+02:00</updated><author><name></name></author><id>tag:nb.asorge.de,2014-08-25:sectioning-and-bootstrapping.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.ipython.org/github/andsor/notebooks/blob/master/notebooks/sectioning-and-bootstrapping.ipynb"&gt;View this notebook in
nbviewer&lt;/a&gt; | &lt;a href="https://github.com/andsor/notebooks/raw/master/notebooks/sectioning-and-bootstrapping.pdf"&gt;Get
PDF&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="sectioning-and-bootstrapping"&gt;Sectioning and bootstrapping&lt;/h1&gt;
&lt;p&gt;The following presentation is based on the excellent textbook by Asmussen and
Glynn &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Given a random element $X$, its distribution $F$, and some real-valued
functional $\psi$, we would like to estimate $\psi(F)$ and its $1-\alpha$
confidence interval without further assumptions.
For example, the mean is the
functional $\psi(F) = \int x F(dx)$, where $dx$ is the probability to find $X$
in $dx$.&lt;/p&gt;
&lt;p&gt;Given $R$ independent samples $X_1, \ldots, X_R$ from $F$, an estimate for
$\psi(F)$ is $\psi(\hat{F}_R)$, where&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_R(dx) := \frac{1}{R} \sum_{r=1}^R \delta_{X_r}(dx)
$$&lt;/p&gt;
&lt;p&gt;is the &lt;em&gt;empirical distribution&lt;/em&gt; and $\delta_{X_r}(A) = 1 \Leftrightarrow X_r
\in A$.&lt;/p&gt;
&lt;p&gt;For real-valued random variables, the empirical cumulative distribution
function is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_R(x) := \frac{1}{R} \sum_{r=1}^R \mathbb{1}_{\{X_r \leq x \}}.
$$&lt;/p&gt;
&lt;p&gt;As $R \to \infty$, we have $\psi(\hat{F}_R) \to \psi(F)$ almost surely &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn, 2007&lt;/a&gt;)&lt;/cite&gt;.
Furthermore, we have a central limit theorem such that $\psi(\hat{F}_R)$ is
distributed as $\psi(F) + Y$, where $Y \sim \mathcal{N}(0, \sigma/\sqrt{R})$.&lt;/p&gt;
&lt;h2 id="sectioning"&gt;Sectioning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Sectioning&lt;/em&gt; means splitting the sample into $N$ subsamples (sections) of size
$K$. The empirical distribution of the $n$-th section is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_{n,K}(dx) := \frac{1}{K} \sum_{r=(n-1)K + 1}^{nK} \delta_{X_r}(dx).
$$&lt;/p&gt;
&lt;p&gt;The $1 - \alpha$ confidence interval for the estimator $\psi(\hat{F})$ is&lt;/p&gt;
&lt;p&gt;$$
\psi(\hat{F}) \pm t_{1-\alpha/2} \frac{\hat{\sigma}}{\sqrt{N}},
$$&lt;/p&gt;
&lt;p&gt;where $t_{1-\alpha/2}$ is the critical value of the Student t distribution with
$N-1$ degrees of freedom and the estimator for the variance&lt;/p&gt;
&lt;p&gt;$$
\hat{\sigma}^2 := \frac{1}{N-1} \sum_{n=1}^N \left(\psi(\hat{F}_{n,K}) -
\psi(\hat{F}_R) \right)^2.
$$&lt;/p&gt;
&lt;p&gt;The number of sections needs to be sufficiently large in order for the central
limit theorem to approximately hold.&lt;/p&gt;
&lt;h2 id="bootstrapping"&gt;Bootstrapping&lt;/h2&gt;
&lt;p&gt;When a model for the distribution $F$ is lacking, or too complicated for
statistical inference, bootstrapping methods provide alternatives.
Bootstrapping takes the empirical distribution $\hat{F}_R$ as a surrogate for
the true distribution $F$.
Instead of drawing more samples from $F$, bootstrapping involves resampling
from $\hat{F}_R$.&lt;/p&gt;
&lt;p&gt;The true $1 - \alpha$ confidence interval of the estimator $\psi(\hat{F}_R)$ is&lt;/p&gt;
&lt;p&gt;$$
(\psi(\hat{F}_R) - z_2, \psi(\hat{F}_R) - z_1)
$$&lt;/p&gt;
&lt;p&gt;with the $\alpha/2$ and $1 - \alpha/2$ quantiles $z_1, z_2$&lt;/p&gt;
&lt;p&gt;$$
P(\psi(\hat{F}_R) - \psi(F) &amp;lt; z_1) = P(\psi(\hat{F}_R) - \psi(F) &amp;gt; z_2) =
\frac{\alpha}{2}
$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$
P(\psi(F) \in (\psi(\hat{F}_R) - z_2, \psi(\hat{F}_R) - z_1)) = 1 - \alpha.
$$&lt;/p&gt;
&lt;p&gt;Assuming that $\hat{F}_R \approx F$, the empirical quantiles $z_1^*, z_2^*$
satisfy &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;$$
P_{\hat{F}_R}(\psi(\hat{F}_R) - \psi(F) &amp;lt; z_1^*) =
P_{\hat{F}_R}(\psi(\hat{F}_R) - \psi(F) &amp;gt; z_2^*) =
\frac{\alpha}{2}
$$&lt;/p&gt;
&lt;p&gt;We draw $B$ bootstrap samples of size $R$ from $\hat{F}_R$.
The $b$-th bootstrap sample is $X_{1,b}^*, \ldots, X_{R,b}^*$ with each random
variable $X_{r,b}^*$ drawn independently from $X_1, \ldots, X_R$ with equal
probabilities $P(X_{r,b}^* = X_{r'}) = \frac{1}{R}$.
The empirical distribution of the $b$-th bootstrap sample is&lt;/p&gt;
&lt;p&gt;$$
\hat{F}_{R,b}^*(dx) := \frac{1}{R} \sum_{r=1}^R \delta_{X_{r,b}^*}(dx).
$$&lt;/p&gt;
&lt;p&gt;Then the empirical quantiles $z_1^*, z_2^*$ are the $\lfloor
\frac{\alpha}{2}(B+1) \rfloor$-th and $\lfloor (1 - \frac{\alpha}{2})(B+1)
\rfloor$-th order statistic, respectively, of the $B$ independent and
identically distributed random variables&lt;/p&gt;
&lt;p&gt;$$
\left( \psi(\hat{F}_{R,b}^*) - \psi(\hat{F}_R) \right)_{b=1}^B.
$$&lt;/p&gt;
&lt;p&gt;These quantiles $z_1^*, z_2^*$ approximate the quantiles $z_1, z_2$ of the true
distribution $F$, and hence, yield the approximate $1 - \alpha$ confidence
interval &lt;cite data-cite="Asmussen2007Stochastic"&gt;(&lt;a href="http://dx.doi.org/10.1007/978-0-387-69033-9"&gt;Asmussen &amp;amp; Glynn,
2007&lt;/a&gt;)&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;$$
\left( \psi(\hat{F}_R) - z_2^*, \psi(\hat{F}_R) - z_1^* \right).
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="http://notebooks.asorge.de"&gt;This work&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative
Commons Attribution 4.0 International
License&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</summary><category term="diss"></category><category term="statistics"></category></entry></feed>